---
title: "Milestone Report Coursera Capstone Project"
author: "Kristiaan De Jongh"
date: "07/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Synopsis

This report describes the data cleaning, preparation and analysis process of the
data set used to develop a word prediction app. The app is developed in context of 
the capstone project of [the Data Science Specialization course on Coursera](https://www.coursera.org/specializations/jhu-data-science).

A result of the exploratory text analysis is that to **cover 90%** of the whole corpus, 
only **3394 words (11.4%)** and to **cover 95%** only **5635 words (18.9%)** are relevant.


## Data Set

The data set used can be downloaded [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) and
consists of 3 text files collected from publicly available sources by a web crawler.
After unzipping the data set, the (English) text files can be found under *final/en_US*.

```{r libs, echo=FALSE, message=FALSE}
library(tm)
library(data.table)
library(ggplot2)
library(wordcloud2)
library(ngram)
```

A quick analysis of the text files (number of word, lines and characters):

```
wc -l -w -c final/en_US/*.txt
  899288 37334690 210160014 en_US.blogs.txt
 1010242 34372720 205811889 en_US.news.txt
 2360148 30374206 167105338 en_US.twitter.txt
 4269678 102081616 583077241 total
```

These files are loaded with the [tm package](https://cran.r-project.org/web/packages/tm/tm.pdf) 
and the result is a *Corpus* with 3 text documents:

```{r init, eval=FALSE}
corpus <- VCorpus(DirSource('final/en_US'), 
                  readerControl = list(reader = readPlain, language = 'en_US'))
```

## Data Cleanup

The text files are cleaned in several steps:

1. *Punctuation*: Remove punctuation, special words (URLs, email addresses, tags, ...), white space, stop words, ...
2. *Spell checking*: Remove incorrectly spelled and non-English words.
3. *Stemming*: Reducing inflected words to their word stem.
4. *Blacklist*: Remove profane words.

The goals are to reduce the size of the data and to aggregate the content for 
better analysis.

Since the execution of the cleanup takes several hours, it will not be performed 
when this report is generated. The complete R code can be found [here on GitHub](https://github.com/kristiaan67/CourseraCapstoneProject) and reproduced.

### Punctuation Cleanup

Steps:

1. Transform character encoding to 'latin1' to remove special characters.
2. Convert all characters to lowercase.
3. Remove URLs, email addresses and tags.
4. Remove word and character repeats.
5. Remove punctuation and some special characters.
6. Remove numbers.
7. Remove stop words.

```{r basecleanup, eval=FALSE}
corpus <- tm_map(corpus, content_transformer(function(x) iconv(x, to = "latin1", sub = "")))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(removeInternetStuff))
corpus <- tm_map(corpus, content_transformer(removeRepeats))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(replaceSpecialChars))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords('en'))
```

### Spell Checking

Using the [hungspell package](https://cran.r-project.org/web/packages/hunspell/hunspell.pdf)
each sentence is checked and all words that are incorrectly spelled (or are not English) 
are collected and the removed from the corpus.

```{r spellchecking, eval=FALSE}
wrong_words <- c()
for (c in 1:length(corpus)) {
    wrong_words <- unique(c(wrong_words, unlist(hunspell(corpus[[c]]$content))))
}
corpus <- tm_map(corpus, removeWords, wrong_words)
```

### Stemming

First the white spaces are stripped and then the corpus is stemmed:

```{r stemming, eval=FALSE}
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
```

### Blacklist

A blacklist with profane words is downloaded https://www.cs.cmu.edu/~biglou/resources/bad-words.txt
and these words are removed from the corpus.

```{r blacklist, eval=FALSE}
download.file("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt",
              destfile = "bad-words.txt", method = "curl")
bad_words <- readLines("bad-words.txt")
corpus <- tm_map(corpus, removeWords, bad_words)
```

Finally the cleaned corpus is saved to *output/en_US* (with file name prefix 'final_').

```{r save, eval=FALSE}
file_names <- sapply(corpus, function(x) paste("final_", x$meta[["id"]], sep = ""), USE.NAMES = FALSE)
writeCorpus(corpus, path = 'output/en_US', filenames = file_names)
```

## Data Analysis

Another quick analysis of the cleaned text files (number of word, lines and characters):

```
wc -l -w -c output/en_US/final_*.txt
  899288 16856521 101415920 final_en_US.blogs.txt
 1010242 16329079 101001090 final_en_US.news.txt
 2360148 14046755 79423805 final_en_US.twitter.txt
 4269678 47232355 281840815 total
```

After the cleanup process the total number of words was reduced by 54% (102.081.616 -> 47.232.355 words).

### Word Frequency

First the occurrence of words in the corpus is analyzed:

```{r wordfreqs, cache=TRUE}
corpus <- VCorpus(DirSource('output/en_US', pattern = "final_"), 
                  readerControl = list(reader = readPlain, language = 'en_US'))
corpusDTM <- TermDocumentMatrix(corpus)
word_matrix <- sort(rowSums(as.matrix(corpusDTM)), decreasing = TRUE)
word_data <- data.table(word = names(word_matrix), freq = word_matrix)
```

The word cloud based on the word frequency:

```{r, out.width="100%"}
wordcloud2(data = word_data)
```

The top-20 words being:

```{r, out.width="100%"}
top_words <- 20
ggplot(word_data[1:top_words,], aes(x = factor(word, levels = word), y = freq)) + 
    theme_bw() + theme(legend.position = "none", 
                       panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                       axis.title.x = element_blank(), axis.text.y = element_blank()) + 
    geom_col(fill = "dodgerblue") + 
    geom_text(aes(label = sprintf("%.0f", freq/1000)), nudge_y = 6000) + 
    labs(title = sprintf("Top %d Words (in thousands)", top_words)) + ylab("Frequency")
```

The distribution of the word frequency shows that a lot of words occur less than 
500 times whereas a minority of words covers most of the corpus.

```{r wordfreqdist, cache=TRUE}
tot_words <- nrow(word_data)
tot_freq_words <- sum(word_data$freq)
word_freqs <- word_data %>% mutate(
    bin = ifelse(freq > 20000, "[20001,)",
                 ifelse(freq > 10000, "[10001,20000]",
                        ifelse(freq > 5000, "[5001,1 10000]",
                               ifelse(freq > 2500, "[2501, 5000]",
                                      ifelse(freq > 1000, "[1001, 2500]",
                                             ifelse(freq > 500, "[501, 1000]", "[1, 500]"))))))) %>%
    group_by(bin) %>%
    summarise(counts = n())
```

```{r, out.width="100%"}
ggplot(word_freqs, aes(x = factor(bin, levels = c("[1, 500]", "[501, 1000]", "[1001, 2500]", "[2501, 5000]", "[5001,1 10000]", "[10001,20000]", "[20001,)")), 
                            y = counts)) + 
    theme_bw() + theme(legend.position = "none", 
                       panel.grid.major = element_blank(), 
                       panel.grid.minor = element_blank()) +
    geom_col(fill = "dodgerblue") +
    geom_text(aes(label = sprintf("%d (%.1f%%)", counts, counts*100/tot_words)), nudge_y = 500) +
    labs(title = "Distribution of Word Frequency") + xlab("Frequency") + ylab("Number of Words")
```

### Corpus Coverage

Next it is analyzed how many words are needed to cover a specific percentage of the whole corpus.
This is checked for a coverage of 50%, 75%, 90% and 95%:

```{r wordcoverage, cache=TRUE}
freqs <- c()
percents <- c()
for (coverage in c(0.5, 0.75, 0.9, 0.95)) {
    freq_words <- 0
    num_words <- 0
    while (freq_words < tot_freq_words * coverage) {
        num_words <- num_words + 1
        freq_words <- freq_words + word_data$freq[num_words]
    }
    freqs <- c(freqs, num_words)
    percents <- c(percents, num_words * 100 / tot_words)
}
cover_data <- data.table(coverage = c(0.5, 0.75, 0.9, 0.95), 
                         num_words = freqs, 
                         percents = percents,
                         min_freq = word_data$freq[freqs])
```

```{r, comment="", out.width="100%"}
cover_data
ggplot(cover_data, aes(x = sprintf("%d%%", coverage * 100), y = num_words)) + 
    theme_bw() + theme(legend.position = "none", 
                       panel.grid.major = element_blank(), 
                       panel.grid.minor = element_blank()) + 
    geom_col(fill = "dodgerblue") + 
    geom_text(aes(label = sprintf("%d (%.1f%%)", num_words, percents)), nudge_y = 150) + 
    labs(title = "Word Coverage") + xlab("Coverage (%)") + ylab("Number of Words")
```

Note that to cover 90% of all word instances, only 3394 words (11.4%) and to 
cover 95% 5635 words (18.9%) are relevant.

### Word Phrases

To predict words following a specific word the context of a word in a sentence needs
to be analyzed, i.e. which words follow on which words.

In order to do that 2- and 3-grams of the corpus are built from the concatenation 
of all the sentences of the 3 documents in the corpus:

```{r ngrams, cache=TRUE}
text <- concatenate(sapply(corpus, function(c) concatenate(c$content), USE.NAMES = FALSE))

ng2 <- ngram(text, n = 2)
pt2 <- as.data.table(get.phrasetable(ng2))

ng3 <- ngram(text, n = 3)
pt3 <- as.data.table(get.phrasetable(ng3))
```

The top-10 of most occurring 2-word phrases:

```{r, comment=""}
head(pt2, n = 10)
```

The top-10 of most occurring 3-word phrases:

```{r, comment=""}
head(pt3, n = 10)
```

These phrase tables combined with the information about the most frequent words will be used
to develop the prediction model.